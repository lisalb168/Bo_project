{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Import and downsampling data](#Import-and-downsampling-data)\n",
    "* [Function to log results](#Function-to-log-results)\n",
    "* [Function to print metrics on training data](#Function-to-print-metrics-on-training-data)\n",
    "* [Modeling](#Modeling)\n",
    "    * [1. Random Forest](#1.-Random-Forest)\n",
    "    * [2. XGboost](#2.-XGboost )\n",
    "    * [3. LightGBM](#3.-LightGBM)\n",
    "    * [4. Logistic Regression with regularizations](#4.-Logistic-Regression-with-regularizations)\n",
    "    * [5. KNN](#5.-KNN)      \n",
    "    * [6. SVM](#6.-SVM)\n",
    "* [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} -c conda-forge lightgbm\n",
    "#!pip install xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import csv\n",
    "import lightgbm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "#from sklearn.metrics import plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and downsampling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to create a balanced training dataset to be used by all ML models that we build shortly. Test sets are still imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_all = pd.read_csv('sm_data.csv')\n",
    "X = data_all.iloc[:,1:]\n",
    "y = data_all['TARGET']\n",
    "\n",
    "# train test split using test_size = 0.2 \n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "X_train_0 = X_train.loc[y_train == 0,:].copy().sample(frac = 0.09)\n",
    "X_train_1 = X_train.loc[y_train == 1,:].copy()\n",
    "\n",
    "X_train_0['target'] = 0\n",
    "X_train_1['target'] = 1\n",
    "\n",
    "# use frac = 1 to randomize the rows\n",
    "X_train_new = X_train_0.append(X_train_1).sample(frac = 1)\n",
    "\n",
    "X_train_new_x = X_train_new.drop(columns='target')\n",
    "y_train_new = X_train_new['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (40209, 350)\n",
      "y train shape:  (40209,)\n",
      "X test shape:  (61501, 350)\n",
      "y test shape:  (61501,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", X_train_new_x.shape)\n",
    "print(\"y train shape: \", y_train_new.shape)\n",
    "print(\"X test shape: \", X_test.shape)\n",
    "print(\"y test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save X_train_new_x, y_train_new, X_test and y_test for future model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new_x.to_csv(\"X_train_new_x.csv\", index=False)\n",
    "y_train_new.to_csv(\"y_train_new.csv\", index=False)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to log results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_log(cv_clf, modelname):\n",
    "    rlt_dict = {}\n",
    "\n",
    "    rlt_dict['best_estimator_'] = [cv_clf.best_estimator_]\n",
    "    rlt_dict['best_params_'] = [cv_clf.best_params_]\n",
    "    rlt_dict['best_score_'] = [cv_clf.best_score_]\n",
    "    rlt_dict['best_index_'] = [cv_clf.best_index_]\n",
    "\n",
    "    rlt_dict['candidate_params'] = [cv_clf.cv_results_['params']]\n",
    "    rlt_dict['mean_test_score'] = [cv_clf.cv_results_['mean_test_score']]\n",
    "    rlt_dict['std_test_score'] = [cv_clf.cv_results_['std_test_score']]\n",
    "    rlt_dict['mean_train_score'] = [cv_clf.cv_results_['mean_train_score']]\n",
    "    rlt_dict['std_train_score'] = [cv_clf.cv_results_['std_train_score']]\n",
    "\n",
    "    rlt_dict['split0_test_score'] = [cv_clf.cv_results_['split0_test_score']]\n",
    "    rlt_dict['split1_test_score'] = [cv_clf.cv_results_['split1_test_score']]\n",
    "    rlt_dict['split2_test_score'] = [cv_clf.cv_results_['split2_test_score']]\n",
    "    rlt_dict['split3_test_score'] = [cv_clf.cv_results_['split3_test_score']]\n",
    "    rlt_dict['split4_test_score'] = [cv_clf.cv_results_['split4_test_score']]\n",
    "\n",
    "    rlt_dict['split0_train_score'] = [cv_clf.cv_results_['split0_train_score']]\n",
    "    rlt_dict['split1_train_score'] = [cv_clf.cv_results_['split1_train_score']]\n",
    "    rlt_dict['split2_train_score'] = [cv_clf.cv_results_['split2_train_score']]\n",
    "    rlt_dict['split3_train_score'] = [cv_clf.cv_results_['split3_train_score']]\n",
    "    rlt_dict['split4_train_score'] = [cv_clf.cv_results_['split4_train_score']]\n",
    "\n",
    "    rlt_df = pd.DataFrame.from_dict(rlt_dict)\n",
    "    \n",
    "    filename = modelname + '_' + 'cv_rlt.csv'\n",
    "    rlt_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to print metrics on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(modelname):\n",
    "    filename = modelname + '_' + 'metrics.csv'\n",
    "    metrics = pd.read_csv(filename)\n",
    "    confusion_matrix = metrics['confusion_matrix'].values[0]\n",
    "    accuracy = metrics['accuracy'].values[0]\n",
    "    auc = metrics['auc'].values[0]\n",
    "    f1_0 = ast.literal_eval(metrics['class_0'].values[0])['f1-score']\n",
    "    f1_1 = ast.literal_eval(metrics['class_1'].values[0])['f1-score']\n",
    "    weighted_avg_f1 = ast.literal_eval(metrics['weighted avg'].values[0])['f1-score']\n",
    "\n",
    "    print(\"1. Confusion Matrix for the training data is: \\n\", confusion_matrix)\n",
    "    print(\"2. Training accuracy: %f\" % (accuracy))\n",
    "    print(\"3. Training AUC: \", auc)\n",
    "    print(\"4. F1 score for non-default group in the training data: \", f1_0)\n",
    "    print(\"5. F1 score for default group in the training data: \", f1_1)\n",
    "    print(\"6. Weighted F1 score for the training data: \", weighted_avg_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 3 models we build are tree based. Since tree based models are robust regardless of the skewed distributions of the features, we do not need to scale the features. Later on when we train other classification models that are more sensitive to the scale of features, such as logistic regression, KNN and SVM, we first transform some of the features before fitting the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "#when max depth is large (eg. 20), it's obvious that the RF model is overfitting \n",
    "#(CV training score 97%, test score 66%)\n",
    "\n",
    "Kfold, niter, verbose, random_state = [5, 5, 0, 123] \n",
    "param_space = {'n_estimators': range(100,500,100), 'max_depth': range(1,10)} \n",
    "\n",
    "clf = RandomForestClassifier(random_state=random_state)\n",
    "cv_clf = RandomizedSearchCV(clf, param_space, cv=Kfold, n_iter=niter, scoring='accuracy', verbose=verbose, n_jobs=-1) \n",
    "cv_clf.fit(X_train_new_x, y_train_new)\n",
    "\n",
    "print('completed in {} s'.format(time.clock() - start))\n",
    "\n",
    "# write out results\n",
    "model_log(cv_clf, 'randomforest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Five candidate parameters are:  [{'n_estimators': 400, 'max_depth': 1}, {'n_estimators': 200, 'max_depth': 5}, {'n_estimators': 200, 'max_depth': 6}, {'n_estimators': 400, 'max_depth': 5}, {'n_estimators': 400, 'max_depth': 4}]\n",
      "2. Best number of trees and depth are: 200 and 6\n",
      "3. Best average CV validation score is:  0.6810664279141486\n",
      "4. Average CV validation score:  [0.6466214  0.67686339 0.68106643 0.67676391 0.67360541]\n",
      "5. Standard Deviation of CV validation score:  [0.0086146  0.00438749 0.00343502 0.00395083 0.00445787]\n",
      "6. Average CV training score:  [0.6494628  0.69391182 0.70619763 0.69428487 0.68429954]\n",
      "7. Standard Deviation of CV training score:  [0.00210377 0.00123042 0.00212855 0.00084375 0.00097449]\n",
      "1st fold validation score:  [0.63918936 0.6695263  0.67760786 0.67064528 0.66654233]\n",
      "2nd fold validation score:  [0.63827406 0.67980602 0.67980602 0.67918428 0.67309127]\n",
      "3rd fold validation score:  [0.64399403 0.67781646 0.68353643 0.67744342 0.67520517]\n",
      "4th fold validation score:  [0.66173362 0.68225345 0.68648178 0.68212909 0.68038801]\n",
      "5th fold validation score:  [0.64991916 0.67491606 0.67790076 0.6744186  0.67280189]\n",
      "1st fold training score:  [0.65034508 0.69613878 0.70925822 0.69570354 0.68547535]\n",
      "2nd fold training score:  [0.64612802 0.69294619 0.70426213 0.69359903 0.68265614]\n",
      "3rd fold training score:  [0.64936115 0.69263531 0.70385799 0.69328815 0.68445923]\n",
      "4th fold training score:  [0.64887466 0.69376399 0.7055148  0.69457225 0.68390948]\n",
      "5th fold training score:  [0.65260507 0.69407486 0.708095   0.69426138 0.68499751]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "result = pd.read_csv('randomforest_cv_rlt.csv')\n",
    "candidate_params = ast.literal_eval(result.candidate_params.values[0])\n",
    "print(\"1. Five candidate parameters are: \", candidate_params)\n",
    "best_n = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['n_estimators']\n",
    "best_d = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['max_depth']\n",
    "print(\"2. Best number of trees and depth are: {} and {}\".format(best_n, best_d))\n",
    "best_score = result.best_score_.values[0]\n",
    "print(\"3. Best average CV validation score is: \", best_score)\n",
    "mean_test_score = result.mean_test_score.values[0]\n",
    "print(\"4. Average CV validation score: \", mean_test_score)\n",
    "std_test_score = result.std_test_score.values[0]\n",
    "print(\"5. Standard Deviation of CV validation score: \", std_test_score)\n",
    "mean_train_score = result.mean_train_score.values[0]\n",
    "print(\"6. Average CV training score: \", mean_train_score)\n",
    "std_train_score = result.std_train_score.values[0]\n",
    "print(\"7. Standard Deviation of CV training score: \", std_train_score)\n",
    "\n",
    "print(\"1st fold validation score: \", result.split0_test_score.values[0])\n",
    "print(\"2nd fold validation score: \", result.split1_test_score.values[0])\n",
    "print(\"3rd fold validation score: \", result.split2_test_score.values[0])\n",
    "print(\"4th fold validation score: \", result.split3_test_score.values[0])\n",
    "print(\"5th fold validation score: \", result.split4_test_score.values[0])\n",
    "print(\"1st fold training score: \", result.split0_train_score.values[0])\n",
    "print(\"2nd fold training score: \", result.split1_train_score.values[0])\n",
    "print(\"3rd fold training score: \", result.split2_train_score.values[0])\n",
    "print(\"4th fold training score: \", result.split3_train_score.values[0])\n",
    "print(\"5th fold training score: \", result.split4_train_score.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cv_clf.predict(X_train_new_x)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_train_new, preds, labels=None, sample_weight=None)\n",
    "report = classification_report(y_train_new, preds, output_dict=True)\n",
    "probs = cv_clf.predict_proba(X_train_new_x)\n",
    "auc = roc_auc_score(y_train_new, probs[:,1])\n",
    "accuracy = float(np.sum(preds==y_train_new))/y_train_new.shape[0] \n",
    "\n",
    "# write out results\n",
    "metrics_dict = {}\n",
    "metrics_dict['auc'] = [auc]\n",
    "metrics_dict['accuracy'] = [accuracy]\n",
    "metrics_dict['confusion_matrix'] = [confusion_matrix]\n",
    "metrics_dict['class_0'] = [report['0']]\n",
    "metrics_dict['class_1'] = [report['1']]\n",
    "metrics_dict['micro avg'] = [report['micro avg']]\n",
    "metrics_dict['macro avg'] = [report['macro avg']]\n",
    "metrics_dict['weighted avg'] = [report['weighted avg']]\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "metrics_df.to_csv('randomforest_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report on the entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Confusion Matrix for the training data is: \n",
      " [[14794  5559]\n",
      " [ 6434 13422]]\n",
      "2. Training accuracy: 0.701733\n",
      "3. Training AUC:  0.7697733933423979\n",
      "4. F1 score for non-default group in the training data:  0.7115749981962916\n",
      "5. F1 score for default group in the training data:  0.6911965393825475\n",
      "6. Weighted F1 score for the training data:  0.7015117119119845\n"
     ]
    }
   ],
   "source": [
    "print_report('randomforest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "Kfold, niter, verbose, random_state = [5, 5, 0, 123] \n",
    "param_space = {'n_estimators': range(100,500,100), 'learning_rate': [0.01,0.1,0.5], \n",
    "               'max_depth': range(1,10), 'gamma': [0.001,0.01,1,10]}\n",
    "               \n",
    "clf = XGBClassifier(objective='binary:logistic', verbosity=verbose, booster='gbtree', tree_method='auto', \n",
    "                            subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, \n",
    "                            reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=random_state)\n",
    "cv_clf = RandomizedSearchCV(clf, param_space, cv=Kfold, n_iter=niter, scoring='accuracy', verbose=verbose, n_jobs=-1)\n",
    "cv_clf.fit(X_train_new_x, y_train_new)\n",
    "\n",
    "print('completed in {} s'.format(time.clock() - start))\n",
    "\n",
    "# write out results\n",
    "model_log(cv_clf, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Five candidate parameters are:  [{'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.5, 'gamma': 1}, {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1, 'gamma': 0.01}, {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.5, 'gamma': 1}, {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.5, 'gamma': 0.01}, {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.001}]\n",
      "2. Best number of trees, learning rate, depth and gamma are: 300, 0.1, 3 and 0.001\n",
      "3. Best average CV validation score is:  0.7073540749583427\n",
      "4. Average CV validation score:  [0.69929618 0.70362357 0.69019374 0.67987267 0.70735407]\n",
      "5. Standard Deviation of CV validation score:  [0.00591636 0.00419497 0.00322462 0.00430646 0.00349034]\n",
      "6. Average CV training score:  [0.74533065 0.88148797 0.82054391 1.         0.74600217]\n",
      "7. Standard Deviation of CV training score:  [0.00200899 0.00268507 0.00137744 0.         0.00166425]\n",
      "1st fold validation score:  [0.68966803 0.69650628 0.68643541 0.67648887 0.70147955]\n",
      "2nd fold validation score:  [0.69709028 0.70591893 0.69298682 0.68179557 0.70604327]\n",
      "3rd fold validation score:  [0.70305894 0.70542154 0.68875902 0.68229296 0.70989804]\n",
      "4th fold validation score:  [0.70725034 0.70861833 0.69493844 0.6734237  0.71160303]\n",
      "5th fold validation score:  [0.6994155  0.70165402 0.68784977 0.68536252 0.70774779]\n",
      "1st fold training score:  [0.74550768 0.87953118 0.82145744 1.         0.74715538]\n",
      "2nd fold training score:  [0.74262443 0.88118258 0.81925576 1.         0.74604408]\n",
      "3rd fold training score:  [0.74846893 0.88550378 0.82040601 1.         0.74800261]\n",
      "4th fold training score:  [0.74384481 0.87792216 0.81895051 1.         0.74309873]\n",
      "5th fold training score:  [0.74620741 0.88330017 0.82264984 1.         0.74571002]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "result = pd.read_csv('xgboost_cv_rlt.csv')\n",
    "candidate_params = ast.literal_eval(result.candidate_params.values[0])\n",
    "print(\"1. Five candidate parameters are: \", candidate_params)\n",
    "best_n = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['n_estimators']\n",
    "best_d = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['max_depth']\n",
    "best_lr = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['learning_rate']\n",
    "best_gamma = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['gamma']\n",
    "print(\"2. Best number of trees, learning rate, depth and gamma are: {}, {}, {} and {}\".format(best_n, best_lr, best_d, best_gamma))\n",
    "best_score = result.best_score_.values[0]\n",
    "print(\"3. Best average CV validation score is: \", best_score)\n",
    "mean_test_score = result.mean_test_score.values[0]\n",
    "print(\"4. Average CV validation score: \", mean_test_score)\n",
    "std_test_score = result.std_test_score.values[0]\n",
    "print(\"5. Standard Deviation of CV validation score: \", std_test_score)\n",
    "mean_train_score = result.mean_train_score.values[0]\n",
    "print(\"6. Average CV training score: \", mean_train_score)\n",
    "std_train_score = result.std_train_score.values[0]\n",
    "print(\"7. Standard Deviation of CV training score: \", std_train_score)\n",
    "\n",
    "print(\"1st fold validation score: \", result.split0_test_score.values[0])\n",
    "print(\"2nd fold validation score: \", result.split1_test_score.values[0])\n",
    "print(\"3rd fold validation score: \", result.split2_test_score.values[0])\n",
    "print(\"4th fold validation score: \", result.split3_test_score.values[0])\n",
    "print(\"5th fold validation score: \", result.split4_test_score.values[0])\n",
    "print(\"1st fold training score: \", result.split0_train_score.values[0])\n",
    "print(\"2nd fold training score: \", result.split1_train_score.values[0])\n",
    "print(\"3rd fold training score: \", result.split2_train_score.values[0])\n",
    "print(\"4th fold training score: \", result.split3_train_score.values[0])\n",
    "print(\"5th fold training score: \", result.split4_train_score.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cv_clf.predict(X_train_new_x)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_train_new, preds, labels=None, sample_weight=None)\n",
    "report = classification_report(y_train_new, preds, output_dict=True)\n",
    "probs = cv_clf.predict_proba(X_train_new_x)\n",
    "auc = roc_auc_score(y_train_new, probs[:,1])\n",
    "accuracy = float(np.sum(preds==y_train_new))/y_train_new.shape[0] \n",
    "\n",
    "# write out results\n",
    "metrics_dict = {}\n",
    "metrics_dict['auc'] = [auc]\n",
    "metrics_dict['accuracy'] = [accuracy]\n",
    "metrics_dict['confusion_matrix'] = [confusion_matrix]\n",
    "metrics_dict['class_0'] = [report['0']]\n",
    "metrics_dict['class_1'] = [report['1']]\n",
    "metrics_dict['micro avg'] = [report['micro avg']]\n",
    "metrics_dict['macro avg'] = [report['macro avg']]\n",
    "metrics_dict['weighted avg'] = [report['weighted avg']]\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "metrics_df.to_csv('xgboost_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report on the entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Confusion Matrix for the training data is: \n",
      " [[15236  5117]\n",
      " [ 5304 14552]]\n",
      "2. Training accuracy: 0.740829\n",
      "3. Training AUC:  0.8167786654290691\n",
      "4. F1 score for non-default group in the training data:  0.7451642090333308\n",
      "5. F1 score for default group in the training data:  0.7363440860215054\n",
      "6. Weighted F1 score for the training data:  0.7408086577258423\n"
     ]
    }
   ],
   "source": [
    "print_report('xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "Kfold, niter, verbose, random_state = [5, 5, 0, 123] \n",
    "param_space = {'n_estimators': range(100,500,100), 'learning_rate': [0.01,0.1,0.5], \n",
    "               'max_depth': range(1,5)}\n",
    "\n",
    "clf = LGBMClassifier(colsample_bytree=1, subsample=1, reg_alpha=0, reg_lambda=1, verbose=verbose)\n",
    "cv_clf = RandomizedSearchCV(clf, param_space, cv=Kfold, n_iter=niter, scoring='accuracy', \n",
    "                            verbose=verbose, n_jobs = -1)  \n",
    "cv_clf.fit(X_train_new_x, y_train_new)\n",
    "\n",
    "print('completed in {} s'.format(time.clock() - start))\n",
    "\n",
    "# write out results\n",
    "model_log(cv_clf, 'lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Five candidate parameters are:  [{'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.1}, {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.01}, {'n_estimators': 400, 'max_depth': 1, 'learning_rate': 0.01}, {'n_estimators': 200, 'max_depth': 2, 'learning_rate': 0.1}, {'n_estimators': 200, 'max_depth': 1, 'learning_rate': 0.5}]\n",
      "2. Best number of trees, learning rate and depth are: 400, 0.1 and 3\n",
      "3. Best average CV validation score is:  0.7077768658757989\n",
      "4. Average CV validation score:  [0.70777687 0.68502077 0.66345843 0.70153448 0.70113656]\n",
      "5. Standard Deviation of CV validation score:  [0.0048596  0.00250395 0.00196091 0.004046   0.00562005]\n",
      "6. Average CV training score:  [0.75340097 0.69401127 0.66440971 0.71316121 0.71107212]\n",
      "7. Standard Deviation of CV training score:  [0.00129703 0.00179467 0.00111364 0.00160364 0.00143365]\n",
      "1st fold validation score:  [0.71590203 0.68929504 0.66666667 0.70806913 0.70719881]\n",
      "2nd fold validation score:  [0.71089281 0.68515295 0.66339219 0.70343198 0.70865456]\n",
      "3rd fold validation score:  [0.70392937 0.68378513 0.66090525 0.69721462 0.69572246]\n",
      "4th fold validation score:  [0.70376819 0.68523815 0.66422087 0.70152966 0.6960577 ]\n",
      "5th fold validation score:  [0.70439    0.68163164 0.6621067  0.69742569 0.69804751]\n",
      "1st fold training score:  [0.75116583 0.69315426 0.66352671 0.71031524 0.70857427]\n",
      "2nd fold training score:  [0.75465539 0.69173376 0.66465632 0.71352628 0.71159884]\n",
      "3rd fold training score:  [0.75446887 0.69540212 0.66636615 0.71492523 0.71147449]\n",
      "4th fold training score:  [0.75394802 0.69304899 0.66317458 0.71272693 0.71294454]\n",
      "5th fold training score:  [0.75276672 0.69671723 0.66432479 0.71431236 0.71076847]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "result = pd.read_csv('lightgbm_cv_rlt.csv')\n",
    "candidate_params = ast.literal_eval(result.candidate_params.values[0])\n",
    "print(\"1. Five candidate parameters are: \", candidate_params)\n",
    "best_n = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['n_estimators']\n",
    "best_d = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['max_depth']\n",
    "best_lr = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['learning_rate']\n",
    "print(\"2. Best number of trees, learning rate and depth are: {}, {} and {}\".format(best_n, best_lr, best_d))\n",
    "best_score = result.best_score_.values[0]\n",
    "print(\"3. Best average CV validation score is: \", best_score)\n",
    "mean_test_score = result.mean_test_score.values[0]\n",
    "print(\"4. Average CV validation score: \", mean_test_score)\n",
    "std_test_score = result.std_test_score.values[0]\n",
    "print(\"5. Standard Deviation of CV validation score: \", std_test_score)\n",
    "mean_train_score = result.mean_train_score.values[0]\n",
    "print(\"6. Average CV training score: \", mean_train_score)\n",
    "std_train_score = result.std_train_score.values[0]\n",
    "print(\"7. Standard Deviation of CV training score: \", std_train_score)\n",
    "\n",
    "print(\"1st fold validation score: \", result.split0_test_score.values[0])\n",
    "print(\"2nd fold validation score: \", result.split1_test_score.values[0])\n",
    "print(\"3rd fold validation score: \", result.split2_test_score.values[0])\n",
    "print(\"4th fold validation score: \", result.split3_test_score.values[0])\n",
    "print(\"5th fold validation score: \", result.split4_test_score.values[0])\n",
    "print(\"1st fold training score: \", result.split0_train_score.values[0])\n",
    "print(\"2nd fold training score: \", result.split1_train_score.values[0])\n",
    "print(\"3rd fold training score: \", result.split2_train_score.values[0])\n",
    "print(\"4th fold training score: \", result.split3_train_score.values[0])\n",
    "print(\"5th fold training score: \", result.split4_train_score.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cv_clf.predict(X_train_new_x)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_train_new, preds, labels=None, sample_weight=None)\n",
    "report = classification_report(y_train_new, preds, output_dict=True)\n",
    "probs = cv_clf.predict_proba(X_train_new_x)\n",
    "auc = roc_auc_score(y_train_new, probs[:,1])\n",
    "accuracy = float(np.sum(preds==y_train_new))/y_train_new.shape[0] \n",
    "\n",
    "# write out results\n",
    "metrics_dict = {}\n",
    "metrics_dict['auc'] = [auc]\n",
    "metrics_dict['accuracy'] = [accuracy]\n",
    "metrics_dict['confusion_matrix'] = [confusion_matrix]\n",
    "metrics_dict['class_0'] = [report['0']]\n",
    "metrics_dict['class_1'] = [report['1']]\n",
    "metrics_dict['micro avg'] = [report['micro avg']]\n",
    "metrics_dict['macro avg'] = [report['macro avg']]\n",
    "metrics_dict['weighted avg'] = [report['weighted avg']]\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "metrics_df.to_csv('lightgbm_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report on the entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Confusion Matrix for the training data is: \n",
      " [[15327  5026]\n",
      " [ 5167 14689]]\n",
      "2. Training accuracy: 0.746500\n",
      "3. Training AUC:  0.8244740404384768\n",
      "4. F1 score for non-default group in the training data:  0.7504590300389257\n",
      "5. F1 score for default group in the training data:  0.7424123726971772\n",
      "6. Weighted F1 score for the training data:  0.7464854313874358\n"
     ]
    }
   ],
   "source": [
    "print_report('lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression with regularizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "Kfold, niter, verbose, random_state = [5, 5, 0, 123] \n",
    "param_space = {'penalty': ['l1','l2'], 'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1, 10, 100]}\n",
    "        \n",
    "clf = LogisticRegression(random_state=random_state)\n",
    "cv_clf = RandomizedSearchCV(clf, param_space, cv=Kfold, n_iter=niter, scoring='accuracy', \n",
    "                            verbose=verbose, n_jobs = -1) \n",
    "cv_clf.fit(X_train_new_x, y_train_new)\n",
    "\n",
    "print('completed in {} s'.format(time.clock() - start))\n",
    "\n",
    "# write out results\n",
    "model_log(cv_clf, 'logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Five candidate parameters are:  [{'penalty': 'l1', 'C': 0.0005}, {'penalty': 'l2', 'C': 100}, {'penalty': 'l2', 'C': 0.1}, {'penalty': 'l1', 'C': 0.01}, {'penalty': 'l1', 'C': 100}]\n",
      "2. Best C and penalty are: 100 and l1\n",
      "3. Best average CV validation score is:  0.6985500758536646\n",
      "4. Average CV validation score:  [0.62565595 0.61869233 0.61859285 0.69350145 0.69855008]\n",
      "5. Standard Deviation of CV validation score:  [0.00548253 0.00579562 0.0044255  0.00465666 0.0068666 ]\n",
      "6. Average CV training score:  [0.62744039 0.61969956 0.62020944 0.69645478 0.70543286]\n",
      "7. Standard Deviation of CV training score:  [0.00117508 0.00076056 0.00180525 0.00098584 0.0016765 ]\n",
      "1st fold validation score:  [0.61830163 0.60872809 0.61059306 0.68581375 0.68979237]\n",
      "2nd fold validation score:  [0.62770455 0.6209898  0.62285501 0.6984581  0.7003233 ]\n",
      "3rd fold validation score:  [0.62148719 0.61626461 0.61713504 0.6982094  0.70940065]\n",
      "4th fold validation score:  [0.63424947 0.62554409 0.62144012 0.69195374 0.70053476]\n",
      "5th fold validation score:  [0.62653899 0.62193757 0.62094267 0.693073   0.69269991]\n",
      "1st fold training score:  [0.62864515 0.61938071 0.62270721 0.69787975 0.7068333 ]\n",
      "2nd fold training score:  [0.62641838 0.6187397  0.62107128 0.695713   0.70528803]\n",
      "3rd fold training score:  [0.62899866 0.61923711 0.62007648 0.69580626 0.70224143]\n",
      "4th fold training score:  [0.62608804 0.62083437 0.61716613 0.69740114 0.70647849]\n",
      "5th fold training score:  [0.62705173 0.62030589 0.62002611 0.69547376 0.70632305]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "result = pd.read_csv('logistic_cv_rlt.csv')\n",
    "candidate_params = ast.literal_eval(result.candidate_params.values[0])\n",
    "print(\"1. Five candidate parameters are: \", candidate_params)\n",
    "best_c = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['C']\n",
    "best_penalty = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['penalty']\n",
    "print(\"2. Best C and penalty are: {} and {}\".format(best_c, best_penalty))\n",
    "best_score = result.best_score_.values[0]\n",
    "print(\"3. Best average CV validation score is: \", best_score)\n",
    "mean_test_score = result.mean_test_score.values[0]\n",
    "print(\"4. Average CV validation score: \", mean_test_score)\n",
    "std_test_score = result.std_test_score.values[0]\n",
    "print(\"5. Standard Deviation of CV validation score: \", std_test_score)\n",
    "mean_train_score = result.mean_train_score.values[0]\n",
    "print(\"6. Average CV training score: \", mean_train_score)\n",
    "std_train_score = result.std_train_score.values[0]\n",
    "print(\"7. Standard Deviation of CV training score: \", std_train_score)\n",
    "\n",
    "print(\"1st fold validation score: \", result.split0_test_score.values[0])\n",
    "print(\"2nd fold validation score: \", result.split1_test_score.values[0])\n",
    "print(\"3rd fold validation score: \", result.split2_test_score.values[0])\n",
    "print(\"4th fold validation score: \", result.split3_test_score.values[0])\n",
    "print(\"5th fold validation score: \", result.split4_test_score.values[0])\n",
    "print(\"1st fold training score: \", result.split0_train_score.values[0])\n",
    "print(\"2nd fold training score: \", result.split1_train_score.values[0])\n",
    "print(\"3rd fold training score: \", result.split2_train_score.values[0])\n",
    "print(\"4th fold training score: \", result.split3_train_score.values[0])\n",
    "print(\"5th fold training score: \", result.split4_train_score.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cv_clf.predict(X_train_new_x)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_train_new, preds, labels=None, sample_weight=None)\n",
    "report = classification_report(y_train_new, preds, output_dict=True)\n",
    "probs = cv_clf.predict_proba(X_train_new_x)\n",
    "auc = roc_auc_score(y_train_new, probs[:,1])\n",
    "accuracy = float(np.sum(preds==y_train_new))/y_train_new.shape[0] \n",
    "\n",
    "# write out results\n",
    "metrics_dict = {}\n",
    "metrics_dict['auc'] = [auc]\n",
    "metrics_dict['accuracy'] = [accuracy]\n",
    "metrics_dict['confusion_matrix'] = [confusion_matrix]\n",
    "metrics_dict['class_0'] = [report['0']]\n",
    "metrics_dict['class_1'] = [report['1']]\n",
    "metrics_dict['micro avg'] = [report['micro avg']]\n",
    "metrics_dict['macro avg'] = [report['macro avg']]\n",
    "metrics_dict['weighted avg'] = [report['weighted avg']]\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "metrics_df.to_csv('logistic_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report on the entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Confusion Matrix for the training data is: \n",
      " [[14531  5822]\n",
      " [ 6050 13806]]\n",
      "2. Training accuracy: 0.704743\n",
      "3. Training AUC:  0.7721142958926439\n",
      "4. F1 score for non-default group in the training data:  0.7099721502907119\n",
      "5. F1 score for default group in the training data:  0.6993212440482222\n",
      "6. Weighted F1 score for the training data:  0.7047125219898123\n"
     ]
    }
   ],
   "source": [
    "print_report('logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "Kfold, niter, verbose, random_state = [5, 5, 0, 123] \n",
    "param_space = {'n_neighbors': range(1,20)}\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "cv_clf = RandomizedSearchCV(clf, param_space, cv=Kfold, n_iter=niter, scoring='accuracy', \n",
    "                            verbose=verbose, n_jobs = -1)\n",
    "cv_clf.fit(X_train_new_x, y_train_new)\n",
    "\n",
    "print('completed in {} s'.format(time.clock() - start))\n",
    "\n",
    "# write out results\n",
    "model_log(cv_clf, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Five candidate parameters are:  [{'n_neighbors': 1}, {'n_neighbors': 8}, {'n_neighbors': 13}, {'n_neighbors': 11}, {'n_neighbors': 15}]\n",
      "2. Best number of neighbors is: 15\n",
      "3. Best average CV validation score is:  0.5600984854137133\n",
      "4. Average CV validation score:  [0.53316422 0.55258773 0.55781044 0.55674103 0.56009849]\n",
      "5. Standard Deviation of CV validation score:  [0.00440273 0.00284203 0.00495116 0.00349181 0.00440329]\n",
      "6. Average CV training score:  [1.         0.66470192 0.64433959 0.65318086 0.63793552]\n",
      "7. Standard Deviation of CV training score:  [0.         0.00096835 0.00116557 0.0014774  0.00129869]\n",
      "1st fold validation score:  [0.53562104 0.54917319 0.55775208 0.55576278 0.55924406]\n",
      "2nd fold validation score:  [0.53233027 0.55595623 0.56080577 0.56229794 0.56366575]\n",
      "3rd fold validation score:  [0.53456852 0.55583188 0.55894056 0.55844317 0.55856752]\n",
      "4th fold validation score:  [0.53811715 0.5519214  0.56298968 0.55540356 0.56585002]\n",
      "5th fold validation score:  [0.52518343 0.55005596 0.54856361 0.55179704 0.55316503]\n",
      "1st fold training score:  [1.         0.66442828 0.64412734 0.65146428 0.63598209]\n",
      "2nd fold training score:  [1.         0.6644698  0.64550626 0.65436628 0.63776541]\n",
      "3rd fold training score:  [1.         0.66350608 0.64525756 0.6522834  0.63922654]\n",
      "4th fold training score:  [1.         0.66463566 0.64222208 0.65235638 0.63721711]\n",
      "5th fold training score:  [1.         0.66646978 0.64458468 0.65543397 0.63948645]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "result = pd.read_csv('knn_cv_rlt.csv')\n",
    "candidate_params = ast.literal_eval(result.candidate_params.values[0])\n",
    "print(\"1. Five candidate parameters are: \", candidate_params)\n",
    "best_k = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['n_neighbors']\n",
    "print(\"2. Best number of neighbors is: {}\".format(best_k))\n",
    "best_score = result.best_score_.values[0]\n",
    "print(\"3. Best average CV validation score is: \", best_score)\n",
    "mean_test_score = result.mean_test_score.values[0]\n",
    "print(\"4. Average CV validation score: \", mean_test_score)\n",
    "std_test_score = result.std_test_score.values[0]\n",
    "print(\"5. Standard Deviation of CV validation score: \", std_test_score)\n",
    "mean_train_score = result.mean_train_score.values[0]\n",
    "print(\"6. Average CV training score: \", mean_train_score)\n",
    "std_train_score = result.std_train_score.values[0]\n",
    "print(\"7. Standard Deviation of CV training score: \", std_train_score)\n",
    "\n",
    "print(\"1st fold validation score: \", result.split0_test_score.values[0])\n",
    "print(\"2nd fold validation score: \", result.split1_test_score.values[0])\n",
    "print(\"3rd fold validation score: \", result.split2_test_score.values[0])\n",
    "print(\"4th fold validation score: \", result.split3_test_score.values[0])\n",
    "print(\"5th fold validation score: \", result.split4_test_score.values[0])\n",
    "print(\"1st fold training score: \", result.split0_train_score.values[0])\n",
    "print(\"2nd fold training score: \", result.split1_train_score.values[0])\n",
    "print(\"3rd fold training score: \", result.split2_train_score.values[0])\n",
    "print(\"4th fold training score: \", result.split3_train_score.values[0])\n",
    "print(\"5th fold training score: \", result.split4_train_score.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cv_clf.predict(X_train_new_x)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_train_new, preds, labels=None, sample_weight=None)\n",
    "report = classification_report(y_train_new, preds, output_dict=True)\n",
    "probs = cv_clf.predict_proba(X_train_new_x)\n",
    "auc = roc_auc_score(y_train_new, probs[:,1])\n",
    "accuracy = float(np.sum(preds==y_train_new))/y_train_new.shape[0] \n",
    "\n",
    "# write out results\n",
    "metrics_dict = {}\n",
    "metrics_dict['auc'] = [auc]\n",
    "metrics_dict['accuracy'] = [accuracy]\n",
    "metrics_dict['confusion_matrix'] = [confusion_matrix]\n",
    "metrics_dict['class_0'] = [report['0']]\n",
    "metrics_dict['class_1'] = [report['1']]\n",
    "metrics_dict['micro avg'] = [report['micro avg']]\n",
    "metrics_dict['macro avg'] = [report['macro avg']]\n",
    "metrics_dict['weighted avg'] = [report['weighted avg']]\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "metrics_df.to_csv('knn_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report on the entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Confusion Matrix for the training data is: \n",
      " [[13202  7151]\n",
      " [ 7346 12510]]\n",
      "2. Training accuracy: 0.639459\n",
      "3. Training AUC:  0.6905650608718249\n",
      "4. F1 score for non-default group in the training data:  0.6455587882936847\n",
      "5. F1 score for default group in the training data:  0.633145228635777\n",
      "6. Weighted F1 score for the training data:  0.6394287268505398\n"
     ]
    }
   ],
   "source": [
    "print_report('knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisalb168/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train_new_x)\n",
    "X_train_new = scaling.transform(X_train_new_x)\n",
    "X_test_new = scaling.transform(X_test)\n",
    "X_train_new = pd.DataFrame(X_train_new)\n",
    "X_train_new.columns = X_train_new_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "#param_space = {'C': [10], 'kernel': ['rbf'], 'gamma': [0.001]}\n",
    "Kfold, niter, verbose, random_state = [2, 2, 10, 123] \n",
    "param_space = {'C': range(1,101), 'kernel': ['linear','rbf','poly'], 'gamma': [0.001, 0.0001]}\n",
    "\n",
    "clf = svm.SVC(random_state=random_state)\n",
    "cv_clf = RandomizedSearchCV(clf, param_space, cv=Kfold, n_iter=niter, scoring='accuracy', \n",
    "                            verbose=verbose, n_jobs = -1) \n",
    "cv_clf.fit(X_train_new.values, y_train_new.values)\n",
    "\n",
    "print('completed in {} s'.format(time.clock() - start))\n",
    "\n",
    "# write out results\n",
    "rlt_dict = {}\n",
    "rlt_dict['best_estimator_'] = [cv_clf.best_estimator_]\n",
    "rlt_dict['best_params_'] = [cv_clf.best_params_]\n",
    "rlt_dict['best_score_'] = [cv_clf.best_score_]\n",
    "rlt_dict['best_index_'] = [cv_clf.best_index_]\n",
    "rlt_dict['candidate_params'] = [cv_clf.cv_results_['params']]\n",
    "rlt_dict['mean_test_score'] = [cv_clf.cv_results_['mean_test_score']]\n",
    "rlt_dict['std_test_score'] = [cv_clf.cv_results_['std_test_score']]\n",
    "rlt_dict['mean_train_score'] = [cv_clf.cv_results_['mean_train_score']]\n",
    "rlt_dict['std_train_score'] = [cv_clf.cv_results_['std_train_score']]\n",
    "\n",
    "rlt_dict['split0_test_score'] = [cv_clf.cv_results_['split0_test_score']]\n",
    "rlt_dict['split1_test_score'] = [cv_clf.cv_results_['split1_test_score']]\n",
    "\n",
    "rlt_dict['split0_train_score'] = [cv_clf.cv_results_['split0_train_score']]\n",
    "rlt_dict['split1_train_score'] = [cv_clf.cv_results_['split1_train_score']]\n",
    "\n",
    "rlt_df = pd.DataFrame.from_dict(rlt_dict)\n",
    "rlt_df.to_csv('kernelsvm_cv_rlt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Two candidate parameters are:  [{'kernel': 'rbf', 'gamma': 0.001, 'C': 52}, {'kernel': 'rbf', 'gamma': 0.0001, 'C': 19}]\n",
      "2. Best C, kernel and gamma are: 52, rbf and 0.001\n",
      "3. Best average CV validation score is:  0.6916610709045239\n",
      "4. Average CV validation score:  [0.69166107 0.68857718]\n",
      "5. Standard Deviation of CV validation score:  [0.00158402 0.00048966]\n",
      "6. Average CV training score:  [0.73112987 0.69292945]\n",
      "7. Standard Deviation of CV training score:  [8.14024966e-04 6.69733006e-05]\n",
      "1st fold validation score:  [0.6900771  0.68808754]\n",
      "2nd fold validation score:  [0.69324513 0.68906685]\n",
      "1st fold training score:  [0.73194389 0.69299642]\n",
      "2nd fold training score:  [0.73031584 0.69286247]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "result = pd.read_csv('kernelsvm_cv_rlt.csv')\n",
    "candidate_params = ast.literal_eval(result.candidate_params.values[0])\n",
    "print(\"1. Two candidate parameters are: \", candidate_params)\n",
    "best_c = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['C']\n",
    "best_kernel = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['kernel']\n",
    "best_gamma = ast.literal_eval(re.search('({.+})', result.best_params_.values[0]).group(0))['gamma']\n",
    "print(\"2. Best C, kernel and gamma are: {}, {} and {}\".format(best_c, best_kernel, best_gamma))\n",
    "best_score = result.best_score_.values[0]\n",
    "print(\"3. Best average CV validation score is: \", best_score)\n",
    "mean_test_score = result.mean_test_score.values[0]\n",
    "print(\"4. Average CV validation score: \", mean_test_score)\n",
    "std_test_score = result.std_test_score.values[0]\n",
    "print(\"5. Standard Deviation of CV validation score: \", std_test_score)\n",
    "mean_train_score = result.mean_train_score.values[0]\n",
    "print(\"6. Average CV training score: \", mean_train_score)\n",
    "std_train_score = result.std_train_score.values[0]\n",
    "print(\"7. Standard Deviation of CV training score: \", std_train_score)\n",
    "\n",
    "print(\"1st fold validation score: \", result.split0_test_score.values[0])\n",
    "print(\"2nd fold validation score: \", result.split1_test_score.values[0])\n",
    "print(\"1st fold training score: \", result.split0_train_score.values[0])\n",
    "print(\"2nd fold training score: \", result.split1_train_score.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cv_clf.predict(X_train_new)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_train_new, preds, labels=None, sample_weight=None)\n",
    "report = classification_report(y_train_new, preds, output_dict=True)\n",
    "\n",
    "# cv_clf does not have predict_proba attribute in SVM\n",
    "# fit the model with the best parameters and use the decision_function attribute of the model to calculate auc\n",
    "clf = svm.SVC(C = best_c, kernel = best_kernel, gamma = best_gamma, random_state = random_state)\n",
    "clf.fit(X_train_new.values, y_train_new.values)\n",
    "\n",
    "probs = clf.decision_function(X_train_new)\n",
    "auc = roc_auc_score(y_train_new, probs)\n",
    "accuracy = float(np.sum(preds==y_train_new))/y_train_new.shape[0] \n",
    "\n",
    "# write out results\n",
    "metrics_dict = {}\n",
    "metrics_dict['auc'] = [auc]\n",
    "metrics_dict['accuracy'] = [accuracy]\n",
    "metrics_dict['confusion_matrix'] = [confusion_matrix]\n",
    "metrics_dict['class_0'] = [report['0']]\n",
    "metrics_dict['class_1'] = [report['1']]\n",
    "metrics_dict['micro avg'] = [report['micro avg']]\n",
    "metrics_dict['macro avg'] = [report['macro avg']]\n",
    "metrics_dict['weighted avg'] = [report['weighted avg']]\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(metrics_dict)\n",
    "metrics_df.to_csv('kernelsvm_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report on the entire training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Confusion Matrix for the training data is: \n",
      " [[14870  5483]\n",
      " [ 5626 14230]]\n",
      "2. Training accuracy: 0.723719\n",
      "3. Training AUC:  0.7965069771949744\n",
      "4. F1 score for non-default group in the training data:  0.7280471982178266\n",
      "5. F1 score for default group in the training data:  0.7192499178649955\n",
      "6. Weighted F1 score for the training data:  0.723702927067442\n"
     ]
    }
   ],
   "source": [
    "print_report('kernelsvm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the final model which has the highest average cross validation accuracy - XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross Validation Accuracy for RandomForest, XGboost, LightGBM, KNN are: 0.6811, 0.7074, 0.7078, 0.6986, 0.5601, 0.6917\n",
      "Best model is:  LightGBM\n"
     ]
    }
   ],
   "source": [
    "def read_cv_results(filename):\n",
    "    result = pd.read_csv(filename)\n",
    "    best_score = result.best_score_.values[0]\n",
    "    return best_score\n",
    "\n",
    "cv_RFT = read_cv_results('randomforest_cv_rlt.csv')\n",
    "cv_XGB = read_cv_results('xgboost_cv_rlt.csv')\n",
    "cv_LGB = read_cv_results('lightgbm_cv_rlt.csv')\n",
    "cv_LGI = read_cv_results('logistic_cv_rlt.csv')\n",
    "cv_KNN = read_cv_results('knn_cv_rlt.csv')\n",
    "cv_SVM = read_cv_results('kernelsvm_cv_rlt.csv')  \n",
    "print(f\"Average Cross Validation Accuracy for RandomForest, XGboost, LightGBM, KNN are: {cv_RFT:.4f}, {cv_XGB:.4f}, {cv_LGB:.4f}, {cv_LGI:.4f}, {cv_KNN:.4f}, {cv_SVM:.4f}\")\n",
    "all_scores = [cv_RFT, cv_XGB, cv_LGB, cv_LGI, cv_KNN, cv_SVM]\n",
    "modelnames = ['RandomForest','XGboost','LightGBM','Logistic','KNN','SVM']\n",
    "print(\"Best model is: \", modelnames[max(enumerate(all_scores),key=(lambda x: x[1]))[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the performance metrics on the entire training data for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metrics_file(filename):\n",
    "    metrics = pd.read_csv(filename)\n",
    "    accuracy = metrics['accuracy'].values[0]\n",
    "    auc = metrics['auc'].values[0]\n",
    "    f1_0 = ast.literal_eval(metrics['class_0'].values[0])['f1-score']\n",
    "    f1_1 = ast.literal_eval(metrics['class_1'].values[0])['f1-score']\n",
    "    return [accuracy, auc, f1_0, f1_1]\n",
    "\n",
    "RFT = read_metrics_file('randomforest_metrics.csv')\n",
    "XGB = read_metrics_file('xgboost_metrics.csv')\n",
    "LGB = read_metrics_file('lightgbm_metrics.csv')\n",
    "LGI = read_metrics_file('logistic_metrics.csv')\n",
    "KNN = read_metrics_file('knn_metrics.csv')\n",
    "SVM = read_metrics_file('kernelsvm_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table below summarizes the entire training performance metrics for each of the models we built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>XGboost</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>Logistic</th>\n",
       "      <th>KNN</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.769773</td>\n",
       "      <td>0.816779</td>\n",
       "      <td>0.824474</td>\n",
       "      <td>0.772114</td>\n",
       "      <td>0.690565</td>\n",
       "      <td>0.796507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.701733</td>\n",
       "      <td>0.740829</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.704743</td>\n",
       "      <td>0.639459</td>\n",
       "      <td>0.723719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-0</th>\n",
       "      <td>0.711575</td>\n",
       "      <td>0.745164</td>\n",
       "      <td>0.750459</td>\n",
       "      <td>0.709972</td>\n",
       "      <td>0.645559</td>\n",
       "      <td>0.728047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-1</th>\n",
       "      <td>0.691197</td>\n",
       "      <td>0.736344</td>\n",
       "      <td>0.742412</td>\n",
       "      <td>0.699321</td>\n",
       "      <td>0.633145</td>\n",
       "      <td>0.719250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RandomForest   XGboost  LightGBM  Logistic       KNN       SVM\n",
       "AUC           0.769773  0.816779  0.824474  0.772114  0.690565  0.796507\n",
       "Accuracy      0.701733  0.740829  0.746500  0.704743  0.639459  0.723719\n",
       "F1-0          0.711575  0.745164  0.750459  0.709972  0.645559  0.728047\n",
       "F1-1          0.691197  0.736344  0.742412  0.699321  0.633145  0.719250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_model_summary = {'RandomForest': {'Accuracy': RFT[0], 'AUC': RFT[1], 'F1-0': RFT[2], 'F1-1': RFT[3]}, \n",
    "                 'XGboost': {'Accuracy': XGB[0], 'AUC': XGB[1], 'F1-0': XGB[2], 'F1-1': XGB[3]},\n",
    "                 'LightGBM': {'Accuracy': LGB[0], 'AUC': LGB[1], 'F1-0': LGB[2], 'F1-1': LGB[3]},\n",
    "                 'Logistic': {'Accuracy': LGI[0], 'AUC': LGI[1], 'F1-0': LGI[2], 'F1-1': LGI[3]},\n",
    "                 'KNN': {'Accuracy': KNN[0], 'AUC': KNN[1], 'F1-0': KNN[2], 'F1-1': KNN[3]},\n",
    "                 'SVM': {'Accuracy': SVM[0], 'AUC': SVM[1], 'F1-0': SVM[2], 'F1-1': SVM[3]}\n",
    "                }\n",
    "training_model_summary = pd.DataFrame(training_model_summary)\n",
    "display(training_model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
